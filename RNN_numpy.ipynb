{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function for dataset creation\n",
    "def data():\n",
    "    inputs = np.random.randint(0, 9, 10)\n",
    "    targets = np.sort(inputs)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 2, 0, 6, 0, 2, 1, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 2, 2, 3, 4, 6, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ENCODER\n",
    "class RNN:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.hidden_layers = 100\n",
    "        self.seq_length = 10\n",
    "        self.learning_rate = 1e-1\n",
    "        self.unique_chars = 10\n",
    "        self.hp = np.zeros((100, 1))\n",
    "        \n",
    "        \n",
    "        self.Wxh = np.random.randn(self.hidden_layers, self.unique_chars) * 0.01 ###input weight(between the input and the hidden state)\n",
    "        self.Whh = np.random.randn(self.hidden_layers, self.hidden_layers) * 0.01 ###hidden state weight(between the previous hidden state and current hidden state)\n",
    "        self.Why = np.random.randn(self.unique_chars, self.hidden_layers) * 0.01 ###output weight(between the hidden state and output)\n",
    "\n",
    "        self.bh = np.zeros((self.hidden_layers, 1))  ##hidden state bias\n",
    "        self.by = np.zeros((self.unique_chars, 1))   ##output state bias\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        x = np.zeros((self.unique_chars, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) +self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.unique_chars), p=p.ravel())\n",
    "            x = np.zeros((self.unique_chars, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        print (ixes)\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.hidden_layers = 100\n",
    "        self.seq_length = 10\n",
    "        self.learning_rate = 1e-1\n",
    "        self.unique_chars = 10\n",
    "        self.hp = np.zeros((100, 1))\n",
    "        \n",
    "        \n",
    "        self.Wxh = np.random.randn(self.hidden_layers, self.unique_chars) * 0.01 ###input weight(between the input and the hidden state)\n",
    "        self.Whh = np.random.randn(self.hidden_layers, self.hidden_layers) * 0.01 ###hidden state weight(between the previous hidden state and current hidden state)\n",
    "        self.Why = np.random.randn(self.unique_chars, self.hidden_layers) * 0.01 ###output weight(between the hidden state and output)\n",
    "\n",
    "        self.bh = np.zeros((self.hidden_layers, 1))  ##hidden state bias\n",
    "        self.by = np.zeros((self.unique_chars, 1))   ##output state bias\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "########DECODER            \n",
    "    def loss_fxn(self, inputs, targets, hp):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "\n",
    "\n",
    "        hs[-1] = np.copy(self.hp)\n",
    "        loss = 0    ##initialize loss as zero\n",
    "\n",
    "        ##forward propagation\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t]  = np.zeros((self.unique_chars, 1))\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            loss += -np.log(ps[t][targets[t], 0]) ##cross-entropy\n",
    "\n",
    "\n",
    "        ###backward propagation\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw) \n",
    "\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        n, p = 0, 0\n",
    "        \n",
    "        mWxh, mWhh, mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        mbh, mby = np.zeros_like(self.bh), np.zeros_like(self.by) # memory variables for Adagrad                                                                                                                \n",
    "        smooth_loss = -np.log(1.0/self.unique_chars)*self.seq_length # loss at iteration 0                                                                                                                        \n",
    "        while n<=1000*10:\n",
    "\n",
    "                                                                                                                                \n",
    "            loss, dWxh, dWhh, dWhy, dbh, dby, hp = self.loss_fxn(inputs, targets, self.hp)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "              # sample from the Encoder model now and then                                                                                                                                                        \n",
    "            if n % 1000 == 0:\n",
    "                print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "                \n",
    "                self.sample(self.hp, inputs[0], len(targets))\n",
    "\n",
    "              # perform parameter update with Adagrad                                                                                                                                                     \n",
    "            for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                                            [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                            [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "                mem += dparam * dparam\n",
    "                param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "            p += self.seq_length # move data pointer                                                                                                                                                         \n",
    "            n += 1 # iteration counter\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = np.zeros((100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn.loss_fxn(inputs, targets, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 23.025848\n",
      "[9, 1, 7, 1, 9, 2, 7, 1, 2, 9]\n",
      "iter 1000, loss: 8.686192\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 2000, loss: 3.197498\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 3000, loss: 1.177809\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 4000, loss: 0.434554\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 5000, loss: 0.160895\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 6000, loss: 0.060054\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 7000, loss: 0.022835\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 8000, loss: 0.009049\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 9000, loss: 0.003903\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n",
      "iter 10000, loss: 0.001951\n",
      "[0, 0, 1, 2, 2, 2, 3, 4, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 2, 0, 6, 0, 2, 1, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 2, 2, 3, 4, 6, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
